{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Red convolucional para clasificacion de tipos de basura\n### Proyecto Parte 2\nRuben Gonzalez 20003314"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creacion de dataset en base a imagenes proporcionadas"},{"metadata":{"trusted":true},"cell_type":"code","source":"def list_dataset():\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n            \n# Add class name prefix to each path based on class name include in filename\ndef add_class_name_prefix(df, col_name):\n    df[col_name] = df[col_name].apply(lambda x: x[:re.search(\"\\d\",x).start()] + '/' + x)\n    return df\n\n\ndef class_id_to_label(id):\n    label_map = {1: 'glass', 2: 'paper', 3: 'cardboard', 4: 'plastic', 5: 'metal', 6: 'trash'}\n    return label_map[id]\n    ","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGES_DIR = '/kaggle/input/garbage-classification/Garbage classification/Garbage classification/'\n    \ntrain_file = '/kaggle/input/garbage-classification/one-indexed-files-notrash_train.txt'\nval_file   = '/kaggle/input/garbage-classification/one-indexed-files-notrash_val.txt'\ntest_file  = '/kaggle/input/garbage-classification/one-indexed-files-notrash_test.txt'\n\ndf_train = pd.read_csv(train_file, sep=' ', header=None, names=['rel_path', 'label'])\ndf_valid = pd.read_csv(val_file,   sep=' ', header=None, names=['rel_path', 'label'])\ndf_test  = pd.read_csv(val_file,   sep=' ', header=None, names=['rel_path', 'label'])\n\ndf_train = add_class_name_prefix(df_train, 'rel_path')\ndf_valid = add_class_name_prefix(df_valid, 'rel_path')\ndf_test  = add_class_name_prefix(df_test,  'rel_path')\n\ndf_train['label'] = df_train['label'].apply(class_id_to_label)\ndf_valid['label'] = df_valid['label'].apply(class_id_to_label)\ndf_test['label']  = df_test['label'].apply(class_id_to_label)\n\nprint(f'Found {len(df_train)} training, {len(df_valid)} validation and {len(df_test)} samples.')","execution_count":3,"outputs":[{"output_type":"stream","text":"Found 1768 training, 328 validation and 328 samples.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"                     rel_path      label\n0  cardboard/cardboard114.jpg  cardboard\n1      plastic/plastic204.jpg    plastic\n2          glass/glass123.jpg      glass\n3          glass/glass152.jpg      glass\n4          glass/glass398.jpg      glass","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rel_path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cardboard/cardboard114.jpg</td>\n      <td>cardboard</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>plastic/plastic204.jpg</td>\n      <td>plastic</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>glass/glass123.jpg</td>\n      <td>glass</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>glass/glass152.jpg</td>\n      <td>glass</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>glass/glass398.jpg</td>\n      <td>glass</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator()\n\ndatagen_train = datagen.flow_from_dataframe(\n    dataframe=df_train,\n    directory=IMAGES_DIR,\n    x_col='rel_path',\n    y_col='label',\n    color_mode=\"rgb\",\n    class_mode=\"categorical\",\n    batch_size=32,\n    shuffle=True,\n    seed=7,\n)\n\ndatagen_valid = datagen.flow_from_dataframe(\n    dataframe=df_valid,\n    directory=IMAGES_DIR,\n    x_col='rel_path',\n    y_col='label',\n    color_mode=\"rgb\",\n    class_mode=\"categorical\",\n    batch_size=32,\n    shuffle=True,\n    seed=7,\n)","execution_count":5,"outputs":[{"output_type":"stream","text":"Found 1768 validated image filenames belonging to 6 classes.\nFound 328 validated image filenames belonging to 6 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, img_path in enumerate(df_train.rel_path.sample(n=6, random_state=7)):\n    img = load_img(IMAGES_DIR+img_path)\n    img = img_to_array(img, dtype=np.uint8)\n    \n    #plt.figure(figsize = (5,5))\n    plt.subplot(2, 3, i+1)\n    plt.imshow(img.squeeze())","execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'plt' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-07a3bbcba50e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#plt.figure(figsize = (5,5))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"]}]},{"metadata":{},"cell_type":"markdown","source":"# Resumen de modelos\n\n## Long short term Memory\ns una arquitectura de red neuronal recurrente artificial (RNN) utilizada en el campo del aprendizaje profundo . A diferencia de las redes neuronales de retroalimentación estándar , LSTM tiene conexiones de retroalimentación que la convierten en una \"computadora de propósito general\" (es decir, puede calcular cualquier cosa que una máquina de Turing pueda). No solo puede procesar puntos de datos individuales (como imágenes), sino también secuencias completas de datos (como voz o video). Una unidad LSTM común se compone de una celda , una puerta de entrada , una puerta de salida y una puerta de olvido . La celda recuerda valores en intervalos de tiempo arbitrarios y las tres puertas regulan el flujo de información dentro y fuera de la celda.\n\n## GRU\nPropone un novedoso modelo llamado RNN Encoder-Decoder que consiste en 2 redes neuronales recurrentes. La primera de estas codifica una secuencia de simbolos en un vector de representacion y la otra decodifica la representacion en otra secuencia de simbolos. Se entrenan de manera conjunta para maximizar la probabilidad condicional de una secuencia dada una secuencia usada como fuente. El rendimiento de un sistema de traduccion estadistica esta fundamentado en mejorar mediante la utilizacion de probabilidades condicionales de un conjunto de frases computadas por la RNN Encoder-Decoder como caracteristica adicional en el actual modelo log-lineal. Cualitativamente se muestra que el modelo propuesto aprende una semantica y sintactica representacion de frases linguisticas.\n\n## Style transfer\nEs un algoritmo utilizado para agregar a cierta imagen un estilo o diseño parecido al de las imagenes que le alimentaron.\n\n## Mask R-CNN\nEs un framework  simple y flexible para segmentacion de instancias. El enfoque utilizado eficientiza la deteccion de objetos en una imagen mientra crea de manera simultanea mascaras de segmentacion de alta calidad. Este metodo extiende Faster R-CNN aggregandole un componente para en paralelo crear una mascara que contenga las cajas utilizadas para identificacion de objetos. \n\n## Retina NET\nConsiste en una simple y unificada red compuesta por una red de backbone y dos subredes enfocadas a tareas especificas. El backbone es responsable de procesar un feature map convolucional sobre las imagenes de entrada.\n\n## Faster R-CNN\nConsiste en una red neuronal convolucional  que se especializa en deteccion de features por regiones de la imagen. Siendo su principal diferenciador con otras redes el hecho de compartir algunas capas convolucionales, reduciendo asi el tiempo para deteccion o clasificacion.\n\n## YOLO\nEs una red neuronal usada para deteccion rapida de objetos. Actualmente se encuentra en su version 3. Es un acronimo de \"You Only look Once\". La gran ventaja y robustes de este metodo es que ha sido entrenado previamente para clasificar y/o detectar un gran numero de objetos y puede ser facilmente configurable para aplicaciones especificas que se necesiten sin afectar en gran manera el rendimiento.\n\n## Inception\nEs un tipo de red neuronal convolucional surgida a partir del visual recognition challenge del 2014. Se centra en un mejor aprovechamiento de los recursos computacionales dentro de la red. Esto lo logra a travez de un minucioso diseño que permite incrementar la profundidad y peso de la red mientras mantiene la carga computacional constante. Las decisiones de arquitectura estan basadas en el principio de Hebbian y la intuicion  del procesamiento multiescala. Un ejemplo de este modelo es GoogleNet usado para clasificacion y deteccion.\n\n## MobileNet\nEs un tipo de arquitectura que utiliza convoluciones separadas de forma inteligente para construir redes neuronales profundas livianas en terminos computacionales. Utiliza 2 hiper parametros globales, la latencia y la precision. \n\n## U-Net\nEs un tipo de modelo para redes neuronales que utiliza mucho el data augmentation para hacer mas eficiente su funcionamiento. La arquitectura consiste en la reduccion del camino para capturar el contexto y un simetrico camino expandido que permite una localizacion precisa.\n\n## DenseNet\nEs un modelo propuesto para CNN en el cual cada capa convolucional esta conectada a todas las demas capas convolucionales como entradas, esto brinda mejoras como la reduccion del problema vanishing del gradiente, fortalece la propagacion de features, refuerza la reutilizacion de features y reduce el numero de parametros.\n\n## ResNet\nConsiste en un modelo que eficientiza el entrenamiento reformulando las capas como funciones residuales que hacen referencia a las capas de entrada en vez de funciones sin referir. este tipo de redes residuales son mas faciles de optimizar y pueden obtener precisiones considerablemente buenas. \n\n## VGG\nConsiste en un modelo que se fundamenta en evaluar la precision de una red convolucional a medida que su profundidad aumenta y su filtro convolucional es pequeño.\n\n\nDado la simplicidad que representa utilizar ResNet50, se usara este modelo en la clasificacion agregandole una capa convolucional al final y una capa oculta seguida de la capa de salida."},{"metadata":{},"cell_type":"markdown","source":"## Creando el modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(num_classes):\n    base_model = ResNet50(weights='imagenet', include_top=False)\n\n    x = base_model.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n    predictions = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n\n    for layer in base_model.layers:\n        layer.trainable = False\n        \n    return model\n\n\nnet = build_model(num_classes=6)\n\nnet.compile(optimizer='Adam',\n            loss='categorical_crossentropy',\n            metrics=[tf.keras.metrics.categorical_accuracy])\n\nnet.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n\nhistory = net.fit_generator(\n    generator=datagen_train,\n    validation_data=datagen_valid,\n    epochs=30,\n    validation_freq=1,\n    callbacks=[early_stop]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Graficando el error y la precision"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 4))\n\naxs[0].plot(history.history['loss'], label='loss')\naxs[0].plot(history.history['val_loss'], label='val_loss')\n\naxs[1].plot(history.history['categorical_accuracy'], label='acc')\naxs[1].plot(history.history['val_categorical_accuracy'], label='val_acc')\n\nplt.legend();\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Guardando el modelo\nDado que ya supero el porcentaje minimo de exactitud solicitada, se procedera a almacenar el modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"net.save('CNN-Modelo.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Probando con datos de test y validando error/precision"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_generator = datagen.flow_from_dataframe(\n    dataframe=df_test,\n    directory=IMAGES_DIR,\n    x_col='rel_path',\n    y_col='label',\n    color_mode=\"rgb\",\n    class_mode=\"categorical\",\n    batch_size=1,\n    shuffle=False,\n    seed=7\n)\n\n# y_pred = net.predict(test_generator, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n\nfilenames = test_generator.filenames\nnb_samples = len(filenames)\n\nmetricas = net.evaluate_generator(test_generator, nb_samples)\nmetricas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Los valores de Loss y Accuracy mostrados, se observa un accuracy de 0.90 lo cual esta arriba del minimo solicitado"},{"metadata":{},"cell_type":"markdown","source":"## Resultado de pruebas realizadas"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cargamos nuevamente el modelo realizado\nfrom keras.models import load_model\nSalida = load_model('CNN-Modelo.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Carga de etiquetas del set de test\nlabels = (test_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\n\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generacion de 16 numeros random de imagenes de prueba\npruebasTest=np.random.randint(0,len(test_generator),16)\npruebasTest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mostrar predicciones y clasificacion real\nplt.figure(figsize=(16, 16))\nfor i, img in enumerate(pruebasTest):\n    a,b = test_generator.__getitem__(pruebasTest[i])\n    PrediceImagen = Salida.predict(a)\n    img = array_to_img(a[0])\n    plt.subplot(4, 4, i+1)\n    plt.title('pred: %s / truth: %s' % (labels[np.argmax(PrediceImagen)], labels[np.argmax(b)]))\n    plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Graficando capas de activacion"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layer_outputs = [layer.output for layer in Salida.layers[:100]]\ntest_image = '../input/cnnclasificacionbasura/descarga.jpg'\nactivation_model = tf.keras.Model(inputs=Salida.input, outputs=layer_outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = image.load_img(test_image, target_size=(299, 299))\nimg_arr = image.img_to_array(img)\nimg_arr = np.expand_dims(img_arr, axis=0)\nimg_arr /= 255.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activations = activation_model.predict(img_arr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,10))\n\nplt.subplot(2,2,1),plt.imshow(activations[1][0, :, :, 1], cmap='plasma')\nplt.title('Activacion 1'), plt.xticks([]), plt.yticks([])\n\nplt.subplot(2,2,2),plt.imshow(activations[3][0, :, :, 1], cmap='plasma')\nplt.title('Activacion 3'), plt.xticks([]), plt.yticks([])\n\nplt.subplot(2,2,3),plt.imshow(activations[40][0, :, :, 1], cmap='plasma')\nplt.title('Activacion 40'), plt.xticks([]), plt.yticks([])\n\nplt.subplot(2,2,4),plt.imshow(activations[99][0, :, :, 1], cmap='plasma')\nplt.title('Activacion 99'), plt.xticks([]), plt.yticks([])\n\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}